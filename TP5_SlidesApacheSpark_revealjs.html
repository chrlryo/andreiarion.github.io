<!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><title>Apache Spark</title><meta content="yes" name="apple-mobile-web-app-capable" /><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style" /><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport" /><link href="reveal.js/css/reveal.css" rel="stylesheet" /><link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css" /><link href="reveal.js/plugin/highlight/styles/sunburst.css" rel="stylesheet" /><script type="text/javascript">document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );</script></head><body><div class="reveal"><div class="slides"><section><h1>Apache Spark</h1><p><small></small></p></section><section id="plan"><h2>Plan</h2><div class="olist arabic"><ol class="arabic"><li><p><strong>Context: Distributed programming</strong></p></li><li><p>Spark computing model</p></li><li><p>Spark programming</p></li><li><p>Spark execution model</p></li><li><p>Spark cluster</p></li><li><p>Spark ecosystem</p></li><li><p>Spark vs MapReduce</p></li><li><p>(Spark on AWS ES Cluster)</p></li></ol></div></section>
<section id="plan-2"><h2>Plan</h2><div class="ulist"><ul><li><p>Distributed programming models</p><div class="ulist"><ul><li><p>Data flow models (MapReduce, Apache Spark)</p></li></ul></div></li></ul></div></section>
<section id="scale-up-scale-out"><h2>Scale-Up / Scale-Out</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/scale-Up-scale-Out.jpg" alt="scale Up scale Out" width="80%" /></div></div>
<div class="ulist"><ul><li><p>Scale-Up: "single powerfull computer"($$$)</p></li><li><p>Scale-Out: network of commodity hardware</p></li></ul></div></section>
<section id="distributed-databases-vs-distributed-programming"><h2>(Distributed) Databases vs Distributed programming</h2><div class="ulist"><ul><li><p>Databases: used as a shared ressource</p><div class="ulist"><ul><li><p>data persistence</p></li><li><p>standardized access to data (API / query language)</p></li><li><p>guaranteed properties (CAP) via sharding and replication</p></li></ul></div></li><li><p>Distributed programming:</p><div class="ulist"><ul><li><p>express general computation</p></li><li><p>pipelines of tasks</p></li></ul></div></li></ul></div></section>
<section id="databases-vs-distributed-programming"><h2>Databases vs Distributed programming</h2><div class="paragraph"><p><strong>More and more difficult to distinguish between them !</strong></p></div>
<div class="ulist"><ul><li><p>Distributed Databases:</p><div class="ulist"><ul><li><p>User Defines Types, User Defined Functions, MapReduce support</p></li></ul></div></li><li><p>Distributed programming frameworks:</p><div class="ulist"><ul><li><p>SQL like access (SparkSQL, Dataframes on top of RDDs), DSLs as query languages, DB inspired optimizations</p></li><li><p>Data persistence, partitioning and replication (MEMORY_ONLY_2)</p></li></ul></div></li></ul></div></section>
<section id="distributed-programming-models"><h2>Distributed programming models</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/SharedMemoryDistributedMemory.png" alt="SharedMemoryDistributedMemory" width="100%" /></div></div></section>
<section id="shared-memory-shared-nothing"><h2>Shared Memory / Shared Nothing</h2><div class="ulist"><ul><li><p><strong>Von Neuman architecture</strong> &#8658; instructions executed sequentially by a worker (CPU) and data does not move</p></li><li><p><strong>Shared Memory</strong></p><div class="ulist"><ul><li><p>multiple workers (CPU/Cores) executing computations in parallel</p></li><li><p>use <strong>locks, semaphores and monitors</strong> to synchronise access to the shared memory</p></li></ul></div></li><li><p><strong>Shared Nothing</strong>: <strong>Divide&amp;Conquer</strong> (ForkJoin/ScatterGather)</p><div class="ulist"><ul><li><p><strong>distribute data</strong></p></li><li><p><strong>distribute the computations</strong> ( <strong>the code</strong>)</p></li><li><p><strong>launch and manage parallel computations</strong></p></li><li><p><strong>collect</strong> results</p></li></ul></div></li></ul></div></section>
<section id="shared-nothing-hegemony"><h2>Shared Nothing hegemony</h2><div class="paragraph"><p>Since 2006, no Shared-Memory system in the first 10 places on TOP500</p></div>
<div class="imageblock" style="float: center"><div class="content"><img src="images/Top500.png" alt="Top500" width="100%" /></div></div></section>
<section id="distributed-programming"><h2>Distributed programming:</h2><div class="admonitionblock warning"><table><tr><td class="icon"><i class="fa fa-warning" title="Warning"></i></td><td class="content"></td></tr></table></div>
<div class="ulist"><ul><li><p>unreliable network</p></li><li><p>hardware/software failures</p></li><li><p>synchronous &#8658; <strong>asynchronous</strong></p></li><li><p>consistency and ordering are expensive</p></li><li><p><strong>time</strong></p></li></ul></div></section>
<section id="share-nothing-architectures"><h2>Share Nothing Architectures</h2><div class="olist arabic"><ol class="arabic"><li><p>Message Passing (MPI, Actors, CSP)</p></li><li><p>DataFlow systems</p></li></ol></div></section>
<section id="share-nothing-architectures-message-passing"><h2>Share Nothing Architectures &#8594; Message Passing</h2><div class="paragraph"><p>Computing grid : <em>MPI (message passing interface)</em> (1993-2012)</p></div>
<div class="ulist"><ul><li><p>launch the same code on multiple nodes</p></li></ul></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash"> mpirun -np 4 simple1</code></pre></div></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="c language-c">  // determining who am I
  int myrank;
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank); <i class="conum" data-value="1"></i><b>(1)</b>
  // determining who are they :
  int nprocs;
  MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);  <i class="conum" data-value="2"></i><b>(2)</b>
  // sending messages
  MPI_Send(buffer, count, datatype, destination, tag, MPI_COMM_WORLD); <i class="conum" data-value="3"></i><b>(3)</b>
  //receiving Messages
  MPI_Recv(buffer, maxcount, datatype, source, tag, MPI_COMM_WORLD, &amp;status); <i class="conum" data-value="4"></i><b>(4)</b></code></pre></div></div></section>
<section id="traditional-distributed-programming"><h2>"Traditional" distributed programming</h2><div class="ulist"><ul><li><p>Low level</p><div class="ulist"><ul><li><p>manual data decomposition</p></li><li><p>manual load balancing</p></li></ul></div></li><li><p>Very difficult to do at scale:</p><div class="ulist"><ul><li><p>How to efficiently split data/code across nodes?</p><div class="ulist"><ul><li><p>Must consider network &amp; data locality</p></li></ul></div></li><li><p>How to deal with failures? (inevitable at scale)</p></li></ul></div></li></ul></div></section>
<section id="the-present-data-flow-models"><h2>The Present: Data flow models</h2><div class="paragraph"><p>Restrict the programming interface so that the
<strong>system</strong> can do more automatically</p></div>
<div class="ulist"><ul><li><p>Express jobs as <strong>graphs of high-level operators</strong></p><div class="ulist"><ul><li><p><strong>System</strong> picks how to split each operator into tasks
and where to run each task</p></li><li><p>Tasks executed on <strong>workers</strong> as soon as soon as input available</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>???</p></div></aside></section>
<section id="data-flow-models-map-reduce"><h2>Data flow models: Map-Reduce</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/map-reduce-step.png" alt="map reduce step" width="50%" /></div></div>
<div class="paragraph"><p><span class="small">&#169; <a href="http://commons.wikimedia.org/wiki/File:Mapreduce.png" class="bare">http://commons.wikimedia.org/wiki/File:Mapreduce.png</a></span></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>tasks are independent map-reduce jobs
*</p></li></ul></div></aside></section>
<section id="map-reduce-strong-strong-points-strong"><h2>Map Reduce &#8658; <strong>strong points:</strong></h2><div class="ulist"><ul><li><p>Simpler programming model</p><div class="ulist"><ul><li><p>High-level functions instead of message passing</p></li></ul></div></li><li><p>Scalability to very largest clusters</p><div class="ulist"><ul><li><p>very good performance for simple jobs (one pass algorithms)</p><div class="ulist"><ul><li><p>Run parts twice fault recovery</p></li></ul></div></li></ul></div></li></ul></div></section>
<section id="map-reduce-iterations"><h2>Map Reduce iterations</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/map-reduce-iterations1.png" alt="map reduce iterations1" width="100%" /></div></div></section>
<section id="map-reduce-iterations-2"><h2>Map Reduce iterations</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/map-reduce-iterations2.png" alt="map reduce iterations2" width="100%" /></div></div></section>
<section id="map-reduce-strong-weak-points-strong"><h2>Map Reduce &#8658; <strong>weak points:</strong></h2><div class="ulist"><ul><li><p>not apropriate for multi-pass /iterative algorithms</p><div class="ulist"><ul><li><p>No efficient primitives for data sharing</p></li><li><p>State between steps goes to distributed file system</p></li></ul></div></li><li><p>cumbersome to write</p></li></ul></div></section>
<section id="apache-spark"><h2>Apache Spark</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-timeline.png" alt="spark timeline" width="100%" /></div></div></section>
<section id="spark-code-size"><h2>Spark code size</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-code-size.png" alt="spark code size" width="100%" /></div></div></section>
<section id="spark-project-activity"><h2>Spark project activity</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-project-activity.png" alt="spark project activity" width="100%" /></div></div></section>
<section id="spark-contributors"><h2>Spark contributors</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-contributors.png" alt="spark contributors" width="100%" /></div></div></section>
<section id="plan-3"><h2>Plan</h2><div class="olist arabic"><ol class="arabic"><li><p>Context: Distributed programming</p></li><li><p><strong>Spark computing model</strong></p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>RDDs</p></li></ol></div></li><li><p>Spark programming</p></li><li><p>Spark execution model</p></li><li><p>Spark cluster</p></li><li><p>Spark ecosystem</p></li><li><p>Spark vs MapReduce</p></li><li><p>(Spark on AWS ES Cluster)</p></li></ol></div></section>
<section id="spark-idea"><h2>Spark Idea</h2><div class="paragraph"><p><strong><em>Distributed programming should be no different than standard programming</em></strong></p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val data = 1 to 1000
val filteredData= data.filter( _%2==0)</code></pre></div></div></section>
<section id="spark-idea-2"><h2>Spark Idea</h2><div class="paragraph"><p><strong><em>Distributed programming should be no different than standard programming</em></strong></p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val data = 1 to 1000
val filteredData = sc.parallelize(data).filter(_%2==0) <i class="conum" data-value="1"></i><b>(1)</b> <i class="conum" data-value="2"></i><b>(2)</b>
filteredData.collect <i class="conum" data-value="3"></i><b>(3)</b></code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>distribute data to nodes</td></tr><tr><td><i class="conum" data-value="2"></i><b>2</b></td><td>distributed filtering</td></tr><tr><td><i class="conum" data-value="3"></i><b>3</b></td><td>collect the result from nodes</td></tr></table></div></section>
<section id="resilient-distributed-datasets-rdds"><h2>Resilient Distributed Datasets (RDDs)</h2><div class="ulist"><ul><li><p>Collections of objects across a cluster with user
controlled partitioning &amp; storage (memory, disk, &#8230;&#8203;)</p></li></ul></div></section>
<section id="building-rdds"><h2>Building RDDs</h2><div class="ulist"><ul><li><p>Built via :</p><div class="ulist"><ul><li><p>paralelization of data (<strong>sc.parallelize</strong>)</p></li><li><p>reading from parallel data sources (hdfs, s3, Cassandra)</p></li><li><p>parallel transformations of other RDDs(map, filter, …)</p></li></ul></div></li><li><p>Automatically rebuilt on failure (lineage)</p></li></ul></div></section>
<section id="resilient-distributed-datasets-rdds-2"><h2>Resilient Distributed Datasets (RDDs)</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/RDDs.png" alt="RDDs" width="100%" /></div></div></section>
<section id="resilient-distributed-datasets-rdds-3"><h2>Resilient Distributed Datasets (RDDs)</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/RDDs-failure.png" alt="RDDs failure" width="100%" /></div></div></section>
<section id="plan-4"><h2>Plan</h2><div class="olist arabic"><ol class="arabic"><li><p>Distributed programming</p></li><li><p>Spark computing model</p></li><li><p><strong>Spark programming</strong></p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>Data scientist vs Data engineer</p></li><li><p>Scala crash course</p></li><li><p>Spark Hello World</p></li></ol></div></li><li><p>Spark execution model</p></li><li><p>Spark cluster</p></li><li><p>Spark ecosystem</p></li><li><p>Spark vs MapReduce</p></li><li><p>(Spark on AWS ES Cluster)</p></li></ol></div></section>
<section id="spark"><h2>Spark</h2><div class="olist arabic"><ol class="arabic"><li><p>A <strong><em>general pourpose computation engine</em></strong>:</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>distribute data</p></li><li><p>distribute computation</p></li></ol></div></li><li><p>Lots of extensions on the side: machine learning, graph procesing, SparkSQL</p></li><li><p>Can be used directly by data scientists and data engineers</p></li></ol></div></section>
<section id="data-scientist-data-engineer-roles"><h2>Data scientist - Data engineer (Roles)</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/data-scientist-vs-data-engineer.jpg" alt="data scientist vs data engineer" width="30%" /></div></div>
<table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p><strong>Data scientist</strong>:</p></div>
<div class="ulist"><ul><li><p>ask the right questions about data</p></li><li><p>perform data analysis</p></li><li><p>create statisticals or mathematical models</p></li><li><p>presents results</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p><strong>Data engineer</strong>:</p></div>
<div class="ulist"><ul><li><p>enable data scientists to do their job effectively</p></li><li><p>manage the data workflow (collection, storage, processing )</p></li><li><p>serves the data via un API to a data scientist to easily querying it</p></li></ul></div></div></td></tr></tbody></table>
<div class="paragraph"><p><span class="small"><a href="http://101.datascience.community/">101.datascience</a></span></p></div></section>
<section><section id="data-scientist-data-engineer-languages"><h2>Data scientist - Data engineer (Languages)</h2><div class="olist arabic"><ol class="arabic"><li><p>Data scientist:</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>R</p></li><li><p>Python</p></li><li><p>Matlab&#8230;&#8203;</p></li></ol></div></li><li><p>Data engineer:</p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>Java/C</p></li></ol></div></li></ol></div></section><section id="scala"><h2>&#8658; &#8658; &#8658; &#8658; Scala</h2><div class="paragraph"><p><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala Crash Course - UMA</a>
<a href="scala_crash_course.pdf">Extract</a></p></div></section></section>
<section id="why-scala"><h2>Why Scala</h2><div class="ulist"><ul><li><p>Spark itself is written in Scala:</p><div class="ulist"><ul><li><p>lambdas</p></li><li><p>extensions (paralel processing, dsl)</p></li></ul></div></li><li><p>Documentation/tutorials</p></li><li><p>Scala/Python only tools</p><div class="ulist"><ul><li><p>Spark-shell</p><div class="ulist"><ul><li><p>inline test of data processing algorithms</p></li></ul></div></li><li><p>Spark notebooks: shell on steroids &#8658; data analytics dashboards</p></li></ul></div></li></ul></div>
<div class="imageblock" style="float: center"><div class="content"><img src="images/spark-languages.png" alt="spark languages" width="20%" /></div></div></section>
<section id="hello-world-in-spark"><h2>Hello world in Spark</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val data = 1 to 1000 <i class="conum" data-value="1"></i><b>(1)</b>
val firstRDD = sc.parallelize(data) <i class="conum" data-value="2"></i><b>(2)</b>
val secondRDD = firstRDD.filter( _ &lt; 10) <i class="conum" data-value="3"></i><b>(3)</b>
secondRDD.collect <i class="conum" data-value="4"></i><b>(4)</b></code></pre></div></div></section>
<section id="hello-world-in-spark-2"><h2>Hello world in Spark</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val data = 1 to 1000 <i class="conum" data-value="1"></i><b>(1)</b></code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>Local data generation</td></tr></table></div></section>
<section id="hello-world-in-spark-3"><h2>Hello world in Spark</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val firstRDD = sc.parallelize(data) <i class="conum" data-value="1"></i><b>(1)</b></code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>Dispatch</td></tr></table></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">def parallelize[T](seq: Seq[T], numSlices: Int): rdd.RDD[T]</code></pre></div></div></section>
<section id="hello-world-in-spark-4"><h2>Hello world in Spark</h2><div class="ulist"><ul><li><p>(distributed) filtering</p></li></ul></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val secondRDD = firstRDD.filter( _ &lt; 10)</code></pre></div></div></section>
<section id="hello-world-in-spark-5"><h2>Hello world in Spark</h2><div class="ulist"><ul><li><p>collect the results</p></li></ul></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">secondRDD.collect <i class="conum" data-value="4"></i><b>(4)</b></code></pre></div></div></section>
<section id="plan-5"><h2>Plan</h2><div class="olist arabic"><ol class="arabic"><li><p>Distributed programming</p></li><li><p>Spark computing model</p></li><li><p>Spark programming</p></li><li><p><strong>Spark execution model</strong></p></li><li><p>Spark cluster</p></li><li><p>Spark ecosystem</p></li><li><p>Spark vs MapReduce</p></li><li><p>(Spark on AWS ES Cluster)</p></li></ol></div></section>
<section id="spark-deconstructed"><h2>Spark deconstructed</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/itas-workshop.png" alt="itas workshop" width="100%" /></div></div>
<div class="paragraph"><p><a href="http://training.databricks.com/workshop/itas_workshop.pdf">Databricks training workshop</a></p></div>
<div class="paragraph"><p><a href="itas_workshop_spark_deconstructed.pdf" target="_blank">Extrait</a></p></div></section>
<section id="spark-word-count"><h2>Spark Word Count</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val input = sc.textFile("s3://...") <i class="conum" data-value="1"></i><b>(1)</b>
val words = input.flatMap(x =&gt; x.split(" ")) <i class="conum" data-value="2"></i><b>(2)</b>
val result = words.map(x =&gt; (x, 1))<i class="conum" data-value="3"></i><b>(3)</b>
                  .reduceByKey((x, y) =&gt; x + y) <i class="conum" data-value="4"></i><b>(4)</b></code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>construit un input RDD pour lire le contenu d&#8217;un fichier depuis AWS S3</td></tr><tr><td><i class="conum" data-value="2"></i><b>2</b></td><td>transformer le RDD input pour construir un RDD qui contient tous les mots du fichier</td></tr><tr><td><i class="conum" data-value="3"></i><b>3</b></td><td>construit un RDD qui contient des paires (mots,1)</td></tr><tr><td><i class="conum" data-value="4"></i><b>4</b></td><td>construit un RDD qui fait</td></tr></table></div></section>
<section id="spark-word-count-optimisation"><h2>Spark Word Count - optimisation</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val input = sc.textFile("s3://...") <i class="conum" data-value="1"></i><b>(1)</b>
val result = lines.flatMap(x =&gt; x.split(" ")).countByValue() <i class="conum" data-value="2"></i><b>(2)</b></code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>construit un input RDD pour lire le contenu d&#8217;un fichier depuis AWS S3</td></tr><tr><td><i class="conum" data-value="2"></i><b>2</b></td><td>utilisation de countByValue</td></tr></table></div></section>
<section id="spark-word-count-java"><h2>Spark Word Count - (Java)</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="kava language-kava">JavaRDD&lt;String&gt; input = sc.textFile("s3://...")
JavaRDD&lt;String&gt; words = rdd.flatMap(new FlatMapFunction&lt;String, String&gt;() {
  public Iterable&lt;String&gt; call(String x) { return Arrays.asList(x.split(" ")); }
});
JavaPairRDD&lt;String, Integer&gt; result = words.mapToPair(
  new PairFunction&lt;String, String, Integer&gt;() {
    public Tuple2&lt;String, Integer&gt; call(String x) { return new Tuple2(x, 1); }
}).reduceByKey(
  new Function2&lt;Integer, Integer, Integer&gt;() {
    public Integer call(Integer a, Integer b) { return a + b; }
});</code></pre></div></div></section>
<section><section id="spark-programming-in-3-steps"><h2>Spark programming in 3 steps</h2></section><section id="create-rdds"><h2>Create RDDs</h2><div class="ulist"><ul><li><p>Parallelize</p></li><li><p>Reading from file/HDFS/S3</p></li></ul></div></section><section id="transform-rdds"><h2>Transform RDDs</h2></section><section id="actions"><h2>Actions</h2><div class="paragraph"><p><a href="http://spark.apache.org/docs/latest/programming-guide.html" class="bare">http://spark.apache.org/docs/latest/programming-guide.html</a></p></div></section></section>
<section id="plan-6"><h2>Plan</h2><div class="olist arabic"><ol class="arabic"><li><p>Distributed programming</p></li><li><p>Spark computing model</p></li><li><p>Spark programming</p></li><li><p>Spark execution model</p></li><li><p><strong>Spark cluster</strong></p></li><li><p>Spark ecosystem</p></li><li><p>Spark vs MapReduce</p></li><li><p>(Spark on AWS ES Cluster)</p></li></ol></div></section>
<section id="spark-cluster-architecture"><h2>Spark Cluster Architecture</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/cluster-diagram.jpg" alt="cluster diagram" width="100%" /></div></div></section>
<section id="plan-7"><h2>Plan</h2><div class="olist arabic"><ol class="arabic"><li><p>Distributed programming</p></li><li><p>Spark computing model</p></li><li><p>Spark programming</p></li><li><p>Spark execution model</p></li><li><p>Spark cluster</p></li><li><p><strong>Spark ecosystem</strong></p><div class="olist loweralpha"><ol class="loweralpha" type="a"><li><p>Spark Stack</p></li><li><p>SparkSQL</p></li><li><p>Spark and Cassandra</p></li></ol></div></li><li><p>Spark vs MapReduce</p></li><li><p>(Spark on AWS ES Cluster)</p></li></ol></div></section>
<section id="spark-stack"><h2>Spark Stack</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-stack-v2.png" alt="spark stack v2" width="100%" /></div></div></section>
<section><section id="sparksql"><h2>SparkSQL</h2></section><section id="idea-sql-on-top-of-rdds"><h2>Idea: SQL on top of RDDs</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/sparksql.jpg" alt="sparksql" width="50%" /></div></div></section></section>
<section id="sparksql-example"><h2>SparkSQL example</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext._


// Define the schema using a case class.
case class Person(name: String, age: Int)


// Create an RDD of Person objects and register it as a table.
val people = sc.textFile("examples/src/main/resources/
people.txt").map(_.split(",")).map(p =&gt; Person(p(0), p(1).trim.toInt))
people.registerAsTable("people")


// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")


// The results of SQL queries are SchemaRDDs and support all the
// normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
teenagers.map(t =&gt; "Name: " + t(0)).collect().foreach(println)</code></pre></div></div></section>
<section id="cassandra-and-spark"><h2>Cassandra and Spark</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-cassandra.png" alt="spark cassandra" width="70%" /></div></div>
<div class="paragraph"><p><a href="http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/">Planet Cassandra article</a></p></div></section>
<section id="cassandra-and-spark-example"><h2>Cassandra and Spark example</h2><div class="paragraph"><p><a href="https://github.com/datastax/spark-cassandra-connector">Datastax Spark Driver on github</a></p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="scala language-scala">package com.datastax.spark.connector.demo

import org.apache.spark.SparkContext._
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector._

object WordCountDemo extends DemoApp {

  CassandraConnector(conf).withSessionDo { session =&gt;
    session.execute(s"CREATE KEYSPACE IF NOT EXISTS demo WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 }")
    session.execute(s"CREATE TABLE IF NOT EXISTS demo.wordcount (word TEXT PRIMARY KEY, count COUNTER)")
    session.execute(s"TRUNCATE demo.wordcount")
  }

  sc.textFile(words)
    .flatMap(_.split("\\s+"))
    .map(word =&gt; (word.toLowerCase, 1))
    .reduceByKey(_ + _)
    .saveToCassandra("demo", "wordcount")

  // print out the data saved from Spark to Cassandra
  sc.cassandraTable("demo", "wordcount").collect.foreach(println)
  sc.stop()
}</code></pre></div></div></section>
<section id="spark-vs-mapreduce"><h2>Spark vs MapReduce</h2><div class="ulist"><ul><li><p>Performance:</p><div class="ulist"><ul><li><p><em>RDDs</em> <strong><em>in memory</em></strong> <em>Resilient Distributed Datasets</em></p></li><li><p>custom caching on nodes</p></li><li><p>lazy evaluation of the lineage graph &#8658; reduces wait states, better pipelining</p></li><li><p>generational differences in hardware &#8658; off-heap use of large memory spaces</p></li><li><p>lower overhead for starting jobs</p></li><li><p>less expensive shuffles</p></li></ul></div></li><li><p>Easier to use:</p><div class="ulist"><ul><li><p>generalized patterns  &#8658; unified engine for many use cases</p></li><li><p>functional programming / ease of use &#8658; reduction in cost to maintain large apps</p></li></ul></div></li></ul></div></section>
<section id="performance-3x-faster-using-10x-fewer-machines-for-distributed-sort"><h2>Performance: 3X faster using 10X fewer machines for distributed sort</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-sort-record.png" alt="spark sort record" width="100%" /></div></div>
<div class="paragraph"><p><a href="http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">Spark officially sets a new record in large scale sorting</a></p></div></section>
<section id="spark-notebooks"><h2>Spark Notebooks</h2><div class="imageblock" style="float: center"><div class="content"><img src="images/spark-notebook.png" alt="spark notebook" width="100%" /></div></div></section>
<section id="spark-cluster-on-aws"><h2>Spark cluster on AWS</h2><div class="olist arabic"><ol class="arabic"><li><p>install awscli</p></li><li><p>configure</p></li><li><p>create default service roles</p></li><li><p>create a SparkRole to allow cloudwatch access</p></li><li><p>create configure a key pair name for ssh acces - SparkOnAWS</p></li><li><p>ssh to the master node</p></li><li><p>run session</p></li><li><p>terminate cluster</p></li></ol></div></section>
<section id="spark-s3-cluster-on-aws"><h2>Spark S3 cluster on AWS</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">pip install awscli  <i class="conum" data-value="1"></i><b>(1)</b>
aws configure  <i class="conum" data-value="2"></i><b>(2)</b>

root@ns:~# aws configure
AWS Access Key ID [****************MJBQ]:
AWS Secret Access Key [****************9rm9]:
Default region name [us-east-1]:
Default output format [json]:
root@ns:~# aws --debug ec2 describe-instances</code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>install AWS Command Line Interface</td></tr><tr><td><i class="conum" data-value="2"></i><b>2</b></td><td>configure local installation</td></tr></table></div></section>
<section id="start-a-s3-cluster-on-aws"><h2>Start a S3 cluster on AWS</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">aws emr create-cluster --name SparkCluster --ami-version 3.2 --instance-type m3.xlarge --instance-count 3 --service-role EMR_DefaultRole --ec2-attributes KeyName=SparkOnAWS,InstanceProfile=SparkRole --applications Name=Hive --bootstrap-actions Path=s3://support.elasticmapreduce/spark/install-spark <i class="conum" data-value="1"></i><b>(1)</b>

aws emr describe-cluster --cluster-id j-1YHNMQ094QL4W <i class="conum" data-value="2"></i><b>(2)</b></code></pre></div></div>
<div class="colist arabic"><table><tr><td><i class="conum" data-value="1"></i><b>1</b></td><td>start the cluster</td></tr><tr><td><i class="conum" data-value="2"></i><b>2</b></td><td>get status</td></tr></table></div></section>
<section id="connect-to-a-cluster-node"><h2>Connect to a cluster node</h2><div class="olist arabic"><ol class="arabic"><li><p>create a key pair using the AWS interface &#8594; name it SparkOnAWS</p></li></ol></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">mv ~/Downloads/SparkOnAWS.pem ~/.ssh/
chmod 400 ~/.ssh/SparkOnAWS.pem
ssh hadoop@ec2-54-165-117-253.compute-1.amazonaws.com -i ~/.ssh/SparkOnAWS.pem</code></pre></div></div></section>
<section id="run-session"><h2>Run session</h2></section>
<section id="terminate-cluster"><h2>Terminate cluster</h2><div class="olist arabic"><ol class="arabic"><li><p>disconnect from the ssh session</p></li></ol></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">aws emr terminate-clusters --cluster-id j-1YHNMQ094QL4W</code></pre></div></div>
<div class="olist arabic"><ol class="arabic"><li><p>verify the cluster state !</p></li></ol></div></section>
<section id="billing-info"><h2>Billing info</h2><div class="ulist"><ul><li><p><em>3 x m3.xlarge instances x 1h</em> = <strong><em>1.26$</em></strong></p></li></ul></div>
<div class="imageblock" style="float: center"><div class="content"><img src="images/billing.png" alt="billing" width="80%" /></div></div></section>
<section id="tp-spark1"><h2>TP Spark1</h2><div class="paragraph"><p>Vous pouvez télécharger ce document en format pdf: <a href="TP5_TPSpark.pdf"><strong><em>Version 1.0</em></strong></a></p></div></section>
<section id="ressources"><h2>Ressources:</h2><div class="imageblock" style="float: right"><div class="content"><img src="images/spark-book.gif" alt="spark book" width="150px" /></div></div>
<div class="ulist"><ul><li><p><a href="http://shop.oreilly.com/product/0636920028512.do">Learning Spark book</a></p></li><li><p>Spark Camp Workshops</p><div class="ulist"><ul><li><p><a href="http://training.databricks.com/workshop/itas_workshop.pdf">ITAS workshop</a></p></li><li><p><a href="http://spark-summit.org/2014">Slides and videos</a></p></li><li><p><a href="http://spark-summit.org/2014/training">Official Spark Training</a></p></li><li><p><a href="http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-amp-camp-2012-advanced-spark.pdf">Advanced Spark 2012</a></p></li></ul></div></li><li><p>Spark&amp;Cassandra on a standalone cluster</p><div class="ulist"><ul><li><p><a href="http://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html" class="bare">http://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html</a></p></li></ul></div></li><li><p>Spark and AWS</p><div class="ulist"><ul><li><p><a href="http://planetcassandra.org/blog/what-are-all-these-aws-ec2-instance-types/">AWS Instance types guide</a></p></li><li><p><a href="https://aws.amazon.com/articles/4926593393724923">Spark and Hadoop on AWS</a> <a href="http://aws.amazon.com/cli/" class="bare">http://aws.amazon.com/cli/</a></p></li><li><p><a href="http://aws.amazon.com/cli/">Amazon CLI interface</a></p></li><li><p><a href="http://aws.amazon.com/fr/ec2/instance-types/">AWS instance types</a></p></li><li><p><a href="http://blogs.aws.amazon.com/bigdata/post/Tx15AY5C50K70RV/Installing-Apache-Spark-on-an-Amazon-EMR-Cluster">See comments on blog.aws.amazon.com</a></p></li></ul></div></li><li><p>Additional ressources:</p><div class="ulist"><ul><li><p><a href="http://lintool.github.io/SparkTutorial/">UM Spark tutorial</a></p></li><li><p><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course</a></p></li></ul></div></li></ul></div></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script type="text/javascript">// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display controls in the bottom right corner
  controls: true,
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: true,
  // Push each slide change to the browser history
  history: true,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  previewLinks: false,
  // Theme (e.g., beige, black, league, night, serif, simple, sky, solarized, white)
  // NOTE setting the theme in the config no longer works in reveal.js 3.x
  //theme: Reveal.getQueryHash().theme || 'simple',
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: Reveal.getQueryHash().transition || 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1024,
  height: 800,

  // Factor of the display size that should remain empty around the content
  margin: 0.2,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
});</script></body></html>